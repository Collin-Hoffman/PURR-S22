"""
Written by Collin Hoffman,some functions are modified from the below document.

3/26/2022 as part of Dr. Gang Shao's research.

preprocessing_csv.py

Original Credits:

Intro_To_NLP_Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16_eYHgw8JM3RyPT9NCyyczDaJRojyrHj

Author: Vishnu Teja Narapareddy

E-mail: vnarapar@purdue.edu

Date: September 27th 2021

# Intro to Natural Language Processing Part 1
"""

#import natural language toolkit
import nltk

#import pandas
import pandas as pd

#download wordnet
nltk.download('wordnet')

#import csv as a pandas dataframe
       
abstract_tags_df = pd.read_csv('PURR_datasets_with_title_abstract_author_and_tags_03182022_.csv', usecols= ['abstract', 'Tags'])

#create vars as dataframe columns
abstract = abstract_tags_df['abstract']
tags = abstract_tags_df['Tags']


'''TOKENIZING '''
#download punkt, creates list of sentences
nltk.download('punkt')

#create array that lematizes the pandas columns
def tokenize_text(text):
    
    #for each row in df, tokenize the words
  for row in range(0, text.shape[0]):
    sentence = text.iloc[row]
    
    #tokenize sentence, replace row with tokenized sentence
    sentence = nltk.word_tokenize(sentence)
    text.iat[row] = sentence
      
  return text

def tokenize_tags(text):
    
  for row in range(0, text.shape[0]):
     sentence = text.iloc[row]
     sentence = sentence.split(",")

     new_sentence = []
     for word in sentence:
       word = word.strip()
       new_sentence.append(word)
       
     text.iat[row] = new_sentence
  return text
    
'''#STOP WORDS REMOVAL'''

#import stopwords in English

nltk.download('stopwords')

from nltk.corpus import stopwords

#has all stop words in english
sw_nltk = stopwords.words('english')


def remove_stopwords(text):
  
    #for all rows, remove stopwords
  for row in range(0, text.shape[0]):
      
      #define a sentence as a row of the df
    sentence = text.iloc[row]
    
    #for each word in the sentence, if is a stopword, remove from sentence
    for word in sentence:
      if word in sw_nltk:
          sentence.remove(word)
    #replace old sentence with new sentence      
    text.iat[row] = sentence

  return text

"""## LOWERCASE"""

def make_text_lowercase(text):
    #make the entire df lowercase
  text = text.str.lower()
    
  return text
    

"""## LEMMATIZATION"""

#define lemmatizer as stem only

lemmatizer = nltk.stem.WordNetLemmatizer()

def replace_text_lemmatization(text):
    
    #for each row, lemmatize the words
  for row in range(0, text.shape[0]):
      
      #create empty list to store lemmatized words
    sentence = text.iloc[row]
    new_sentence = []
    for word in sentence:

    #lemmatize each word, add to new list
      word = lemmatizer.lemmatize(word, 'v')
      new_sentence.append(word)
         
     #replace old row with new lemmatized sentence
    text.iat[row] = new_sentence
  
  return text
      
  
def rejoin_sentences(text):
    #rejoin all sentences, replace old sentence list with combined sentence
    for row in range(0, text.shape[0]):
        sentence = text.iloc[row]
        sentence = ' '.join(sentence)
        text.iat[row] = sentence

    return text

def rejoin_tags(text):
    for row in range(0, text.shape[0]):
        sentence = text.iloc[row]
        sentence = ', '.join(sentence)
        text.iat[row] = sentence

    return text

def num_tags(text):
    
    #create new empty list of unique tags
    unique_tags = []
    
    #set initial sum to 0
    sum_tags = 0
    
    for row in range(0, text.shape[0]):
        
        #add up all words in row
        sentence = text.iloc[row]
        sum_tags = sum_tags + len(sentence)
        
        #add tag to unique_tags if it isn't already in it
        for word in sentence:
            
            if word not in unique_tags:
                unique_tags.append(word)

    #calculate stats
    num_unique = len(unique_tags)
    
    avg_tags = sum_tags / text.shape[0]
    avg_tags = format(avg_tags, '.2f')
    print(f"There are {num_unique} unique tags in the dataset.")
    print(f"There are an average of {avg_tags} per dataset.")


def preprocess_text(text):
    
    #combine all processes together
    text = make_text_lowercase(text)
    text = tokenize_text(text)
    text = remove_stopwords(text)
    text = replace_text_lemmatization(text)
    text = rejoin_sentences(text)
    
    return text

def preprocess_tags(text):
                    
    text = make_text_lowercase(text)
    text = tokenize_tags(text)
    text = remove_stopwords(text)
    text = replace_text_lemmatization(text)
    num_tags(text)
    text = rejoin_tags(text)
    
    return text
    

'''APPLY LOWERCAESE, TOKENIZE, STOPWORDS, LEMMATIZATION'''

txt= preprocess_tags(tags[105:106])
print(txt.iloc[0])

'''
abstract = preprocess_text(abstract)
tags = preprocess_tags(tags)


#create a new dataframe of the existing dataframes
cleaned_df = pd.concat([abstract, tags], axis = 1)

#export to csv file
cleaned_df.to_csv('Cleaned_abstract_tags(1).csv')

'''








